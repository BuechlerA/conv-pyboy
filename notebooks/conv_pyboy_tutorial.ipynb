{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd948ad",
   "metadata": {},
   "source": [
    "# ConvPyBoy: Training AI to Play Tetris\n",
    "\n",
    "This notebook introduces ConvPyBoy, a project that trains reinforcement learning agents to play Tetris using PyBoy (a Game Boy emulator) and convolutional neural networks. We'll explore:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Understanding the CNN architecture\n",
    "3. Training an agent\n",
    "4. Visualizing the agent's performance\n",
    "5. Analyzing the training results\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab269c",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import the necessary packages and modules for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffc951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Make sure the project root is in the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from environment.tetris_env import TetrisEnv\n",
    "from models.cnn_model import TetrisCNN, DuelingTetrisCNN\n",
    "from agents.dqn_agent import DQNAgent\n",
    "from utils.device_utils import get_device, print_device_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159dba0",
   "metadata": {},
   "source": [
    "### Check Compute Device\n",
    "\n",
    "Next, let's check what compute device (CPU or GPU) we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print_device_info()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ef621",
   "metadata": {},
   "source": [
    "## 2. The Tetris Environment\n",
    "\n",
    "The `TetrisEnv` class provides a reinforcement learning-friendly interface to the PyBoy Tetris emulation. Let's create an instance and explore how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a31587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Tetris ROM file\n",
    "ROM_PATH = \"../tetris.gb\"\n",
    "\n",
    "# Create the environment\n",
    "env = TetrisEnv(rom_path=ROM_PATH, headless=False, down_sample=2, frameskip=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f809b",
   "metadata": {},
   "source": [
    "### Environment Parameters\n",
    "\n",
    "- **rom_path**: Path to the Tetris ROM file\n",
    "- **headless**: Whether to run in headless mode (no visualization)\n",
    "- **down_sample**: Factor by which to downsample the screen\n",
    "- **frameskip**: Number of frames to skip between observations\n",
    "\n",
    "### Environment Interface\n",
    "\n",
    "The environment follows the standard RL interface:\n",
    "- `reset()`: Resets the environment and returns the initial observation\n",
    "- `step(action)`: Takes an action and returns (next_state, reward, done, info)\n",
    "- `close()`: Closes the environment\n",
    "\n",
    "Let's see what the initial observation looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment and get initial state\n",
    "initial_state = env.reset()\n",
    "print(f\"Initial state shape: {initial_state.shape}\")\n",
    "\n",
    "# Visualize initial state\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(initial_state)\n",
    "plt.title(\"Tetris Initial State\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35067edf",
   "metadata": {},
   "source": [
    "### Available Actions\n",
    "\n",
    "The environment supports the following actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5247f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available actions:\")\n",
    "for action_id, action_name in env.ACTIONS.items():\n",
    "    print(f\"{action_id}: {action_name}\")\n",
    "\n",
    "print(f\"\\nTotal number of actions: {len(env.ACTIONS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525ba24",
   "metadata": {},
   "source": [
    "### Manual Play Test\n",
    "\n",
    "Let's test the environment by manually playing a few steps with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e14af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_random_steps(env, num_steps=10):\n",
    "    \"\"\"Play a few random steps and visualize the results\"\"\"\n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Play random actions\n",
    "    for step in range(num_steps):\n",
    "        # Choose random action\n",
    "        action = np.random.randint(0, len(env.ACTIONS))\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Accumulate reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Print step info\n",
    "        print(f\"Step {step+1}: Action={env.ACTIONS[action]}, Reward={reward:.2f}, Score={info['score']}, Done={done}\")\n",
    "        \n",
    "        # Visualize state\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(next_state)\n",
    "        plt.title(f\"Step {step+1} - Action: {env.ACTIONS[action]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Break if game is done\n",
    "        if done:\n",
    "            print(\"Game over!\")\n",
    "            break\n",
    "            \n",
    "    print(f\"\\nTotal reward: {total_reward:.2f}\")\n",
    "    \n",
    "# Play 5 random steps\n",
    "play_random_steps(env, num_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20213105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b717b1",
   "metadata": {},
   "source": [
    "## 3. The CNN Model Architecture\n",
    "\n",
    "The project uses convolutional neural networks (CNNs) to process the game screen and predict the best actions. Let's examine the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary environment to get state dimensions\n",
    "env = TetrisEnv(rom_path=ROM_PATH, headless=True, down_sample=2)\n",
    "initial_state = env.reset()\n",
    "env.close()\n",
    "\n",
    "# Get state dimensions\n",
    "input_height, input_width, input_channels = initial_state.shape\n",
    "num_actions = len(env.ACTIONS)\n",
    "\n",
    "print(f\"Input dimensions: {initial_state.shape}\")\n",
    "print(f\"Number of actions: {num_actions}\")\n",
    "\n",
    "# Create the CNN model\n",
    "cnn_model = TetrisCNN(input_channels=input_channels, \n",
    "                       input_height=input_height, \n",
    "                       input_width=input_width, \n",
    "                       num_actions=num_actions)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nCNN Model Architecture:\")\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e0c00",
   "metadata": {},
   "source": [
    "### Model Architecture Explanation\n",
    "\n",
    "The CNN model consists of:\n",
    "\n",
    "1. **Convolutional layers**: Three convolutional layers that extract spatial features from the game screen:\n",
    "   - Conv1: 32 filters with 5×5 kernel, stride 2, padding 2\n",
    "   - Conv2: 64 filters with 4×4 kernel, stride 2, no padding\n",
    "   - Conv3: 64 filters with 3×3 kernel, stride 1, no padding\n",
    "\n",
    "2. **Fully connected layers**: Two fully connected layers that map extracted features to action values:\n",
    "   - FC1: Maps flattened features to 512 units\n",
    "   - FC2: Maps 512 units to action space size\n",
    "\n",
    "Let's also look at the dueling architecture which is an advanced variant of DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Dueling CNN model\n",
    "dueling_cnn = DuelingTetrisCNN(input_channels=input_channels, \n",
    "                               input_height=input_height, \n",
    "                               input_width=input_width, \n",
    "                               num_actions=num_actions)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nDueling CNN Architecture:\")\n",
    "print(dueling_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7b3e1",
   "metadata": {},
   "source": [
    "### Dueling Architecture Explanation\n",
    "\n",
    "The Dueling CNN separates the estimation of the state value and the action advantages, which can lead to better policy evaluation. It consists of:\n",
    "\n",
    "1. **Convolutional layers**: Same as the base CNN model\n",
    "\n",
    "2. **Value stream**: Estimates the value of being in the given state\n",
    "   - FC from 512 to 256 units\n",
    "   - Output layer with a single value\n",
    "\n",
    "3. **Advantage stream**: Estimates the relative advantage of each action\n",
    "   - FC from 512 to 256 units\n",
    "   - Output layer with one value per action\n",
    "\n",
    "The dueling architecture combines these streams to produce better Q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f25477",
   "metadata": {},
   "source": [
    "## 4. DQN Agent Implementation\n",
    "\n",
    "Let's examine the DQN (Deep Q-Network) agent that uses our CNN models for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DQNAgent class\n",
    "from agents.dqn_agent import DQNAgent\n",
    "\n",
    "# Create a DQN agent\n",
    "agent = DQNAgent(\n",
    "    state_shape=(input_channels, input_height, input_width),\n",
    "    num_actions=num_actions,\n",
    "    learning_rate=0.0001,\n",
    "    dueling=True  # Use dueling architecture\n",
    ")\n",
    "\n",
    "print(f\"DQN Agent created with {'dueling' if agent.dueling else 'standard'} architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4e0dd",
   "metadata": {},
   "source": [
    "## 5. Training the Agent\n",
    "\n",
    "Let's implement a simplified training loop to train our agent on the Tetris environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f79b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, num_episodes=10, max_steps=1000, epsilon_start=1.0, epsilon_final=0.1, epsilon_decay=0.995):\n",
    "    \"\"\"Train the agent on the environment\"\"\"\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    scores = []\n",
    "    episode_lengths = []\n",
    "    epsilon_values = []\n",
    "    loss_values = []\n",
    "    \n",
    "    # Current exploration rate\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        # Episode loop\n",
    "        for step in range(1, max_steps + 1):\n",
    "            # Select action\n",
    "            action = agent.act(state, epsilon)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store experience in replay buffer\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Learn from experience\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                loss = agent.learn()\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            # Update state and score\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            # Break if game is done\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update exploration rate\n",
    "        epsilon = max(epsilon_final, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Store metrics\n",
    "        scores.append(score)\n",
    "        episode_lengths.append(step)\n",
    "        epsilon_values.append(epsilon)\n",
    "        loss_values.append(np.mean(episode_loss) if episode_loss else 0)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Episode {episode}/{num_episodes} - Score: {score:.2f}, Steps: {step}, Epsilon: {epsilon:.4f}, Avg Loss: {loss_values[-1]:.6f}\")\n",
    "        \n",
    "        # Plot progress every few episodes\n",
    "        if episode % 5 == 0 or episode == num_episodes:\n",
    "            clear_output(wait=True)\n",
    "            plot_training_results(scores, episode_lengths, epsilon_values, loss_values)\n",
    "            print(f\"Episode {episode}/{num_episodes} - Score: {score:.2f}, Steps: {step}, Epsilon: {epsilon:.4f}, Avg Loss: {loss_values[-1]:.6f}\")\n",
    "    \n",
    "    return scores, episode_lengths, epsilon_values, loss_values\n",
    "\n",
    "def plot_training_results(scores, episode_lengths, epsilon_values, loss_values):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot scores\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(scores)\n",
    "    plt.title('Episode Scores')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    # Plot episode lengths\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(episode_lengths)\n",
    "    plt.title('Episode Lengths')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps')\n",
    "    \n",
    "    # Plot epsilon values\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epsilon_values)\n",
    "    plt.title('Exploration Rate (Epsilon)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    \n",
    "    # Plot loss values\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(loss_values)\n",
    "    plt.title('Average Loss')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce45391",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "\n",
    "Now, let's train our agent on the Tetris environment. Note that this can take significant time and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b085b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new environment for training\n",
    "env = TetrisEnv(rom_path=ROM_PATH, headless=True, down_sample=2, frameskip=4)\n",
    "\n",
    "# Create a new agent\n",
    "agent = DQNAgent(\n",
    "    state_shape=(input_channels, input_height, input_width),\n",
    "    num_actions=num_actions,\n",
    "    learning_rate=0.0001,\n",
    "    memory_size=10000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    target_update=10,\n",
    "    dueling=True\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "scores, episode_lengths, epsilon_values, loss_values = train_agent(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    num_episodes=20,  # Start with a small number for demonstration\n",
    "    max_steps=2000,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_final=0.1,\n",
    "    epsilon_decay=0.95\n",
    ")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fb316",
   "metadata": {},
   "source": [
    "### Save the Trained Agent\n",
    "\n",
    "After training, let's save our agent for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b66862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save directory if it doesn't exist\n",
    "save_dir = \"../data/agents\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for unique filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = os.path.join(save_dir, f\"notebook_agent_{timestamp}.pt\")\n",
    "\n",
    "# Save the agent\n",
    "agent.save(save_path)\n",
    "print(f\"Agent saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b770e0",
   "metadata": {},
   "source": [
    "## 6. Testing the Agent\n",
    "\n",
    "Let's test our trained agent or one of the pre-trained agents on the Tetris environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4399754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to a pre-trained agent\n",
    "pretrained_path = \"../data/agents/best_agent_gen3.pt\"  # Using one of the pre-trained agents\n",
    "\n",
    "# Create a new agent and load the pre-trained weights\n",
    "test_agent = DQNAgent(\n",
    "    state_shape=(input_channels, input_height, input_width),\n",
    "    num_actions=num_actions,\n",
    "    dueling=True\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "test_agent.load(pretrained_path)\n",
    "print(f\"Loaded pre-trained agent from {pretrained_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b9a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_performance(env, agent, num_episodes=3, max_steps=5000, render=True):\n",
    "    \"\"\"Test the agent's performance\"\"\"\n",
    "    all_scores = []\n",
    "    all_steps = []\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        \n",
    "        print(f\"\\nEpisode {episode}/{num_episodes}\")\n",
    "        \n",
    "        for step in range(1, max_steps + 1):\n",
    "            # Choose action with no exploration (epsilon=0)\n",
    "            action = agent.act(state, epsilon=0)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Update total reward and state\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Print progress every 100 steps\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Step {step} - Score: {info['score']}, Reward: {total_reward:.2f}\")\n",
    "                \n",
    "                # Render the current state\n",
    "                if render:\n",
    "                    plt.figure(figsize=(5, 5))\n",
    "                    plt.imshow(next_state)\n",
    "                    plt.title(f\"Episode {episode} - Step {step}\\nScore: {info['score']}\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "            \n",
    "            # Break if game is done\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Record episode stats\n",
    "        all_scores.append(info['score'])\n",
    "        all_steps.append(step)\n",
    "        \n",
    "        print(f\"Episode {episode} finished - Score: {info['score']}, Steps: {step}, Total Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    # Print overall stats\n",
    "    print(f\"\\nOverall performance over {num_episodes} episodes:\")\n",
    "    print(f\"Average score: {np.mean(all_scores):.2f}\")\n",
    "    print(f\"Average steps: {np.mean(all_steps):.2f}\")\n",
    "    print(f\"Best score: {np.max(all_scores)}\")\n",
    "    \n",
    "    return all_scores, all_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6739bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new environment for testing\n",
    "env = TetrisEnv(rom_path=ROM_PATH, headless=False, down_sample=2, frameskip=4)\n",
    "\n",
    "# Test the agent\n",
    "test_scores, test_steps = test_agent_performance(\n",
    "    env=env,\n",
    "    agent=test_agent,\n",
    "    num_episodes=1,  # Test for 1 episode for demonstration\n",
    "    max_steps=2000,\n",
    "    render=True\n",
    ")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651ccfe",
   "metadata": {},
   "source": [
    "## 7. Evolutionary Training\n",
    "\n",
    "This project also supports evolutionary training through the `Population` class. Let's briefly explore how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Population class\n",
    "from agents.population import Population\n",
    "\n",
    "# Create a small population for demonstration\n",
    "population = Population(\n",
    "    state_shape=(input_channels, input_height, input_width),\n",
    "    num_actions=num_actions,\n",
    "    population_size=5,  # Small population for demonstration\n",
    "    mutation_rate=0.05,\n",
    "    mutation_scale=0.1,\n",
    "    elite_count=1,\n",
    "    save_dir=\"../data/agents\",\n",
    "    training_steps_per_episode=16\n",
    ")\n",
    "\n",
    "print(f\"Created population with {population.population_size} agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8929d5d",
   "metadata": {},
   "source": [
    "### Evolutionary Training Process\n",
    "\n",
    "The evolutionary training process implemented in this project follows these steps:\n",
    "\n",
    "1. **Initialize population**: Create a population of agents with random neural networks\n",
    "2. **Evaluate fitness**: Test each agent's performance on the environment\n",
    "3. **Selection**: Select the best-performing agents for reproduction\n",
    "4. **Reproduction**: Create new agents by combining and mutating the selected agents\n",
    "5. **Repeat**: Evaluate the new population and continue the process\n",
    "\n",
    "This approach can efficiently explore the parameter space and find good solutions without requiring gradient information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eddea79",
   "metadata": {},
   "source": [
    "## 8. Analyzing Pre-trained Agents\n",
    "\n",
    "Let's analyze some of the pre-trained agents to understand their performance and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acedc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available pre-trained agents\n",
    "agents_dir = \"../data/agents\"\n",
    "pretrained_agents = [f for f in os.listdir(agents_dir) if f.endswith(\".pt\")]\n",
    "\n",
    "print(\"Available pre-trained agents:\")\n",
    "for i, agent_file in enumerate(pretrained_agents):\n",
    "    print(f\"{i+1}. {agent_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a681d2",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we explored the ConvPyBoy project for training AI agents to play Tetris using PyBoy and reinforcement learning. We covered:\n",
    "\n",
    "1. Setting up the Tetris environment with PyBoy\n",
    "2. Understanding the CNN architecture used for processing game screens\n",
    "3. Training DQN agents to play Tetris\n",
    "4. Testing pre-trained agents\n",
    "5. Introduction to evolutionary training\n",
    "\n",
    "The project demonstrates how deep reinforcement learning can be applied to classic games, combining modern AI techniques with retro gaming emulation. The agents learn to make sense of pixel inputs and develop strategies for playing Tetris effectively.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "If you want to continue exploring this project, here are some suggestions:\n",
    "\n",
    "1. Train agents for longer periods to achieve higher scores\n",
    "2. Experiment with different CNN architectures or hyperparameters\n",
    "3. Try different reinforcement learning algorithms like PPO or A3C\n",
    "4. Apply the same approach to other Game Boy games\n",
    "5. Implement reward shaping to guide the learning process more effectively"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
